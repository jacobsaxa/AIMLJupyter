{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "s25cbeSlGsfG",
        "hKun-sDgVvpK",
        "aCq2-woIVcEE",
        "MKHnqJBeg2MW"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **UIowa CS:4420 Project 1 Deep Learning on FashionMNIST**<br>\n",
        "Alex Jacobs<br>\n",
        "Code created with guidance from tutorials provided by Dr. Muchao Ye for<br>the course.<br>\n",
        "Important Note: This code is tailored for being run on terminals provided by Google Colab."
      ],
      "metadata": {
        "id": "cfyCitdfvI0y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 1**\n",
        "Implementation of CNN<br>\n",
        "Input: 28x28 Greyscale<br>\n"
      ],
      "metadata": {
        "id": "s25cbeSlGsfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torcheval\n",
        "from torcheval.metrics import BinaryAUROC\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the FashionMNIST dataset\n",
        "full_train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data', train=True, download=True, transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data', train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "# Define the neural network for FashionMNIST\n",
        "class FashionMNISTNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionMNISTNet, self).__init__()\n",
        "        #Initial size 28x28\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        #Size 1 = 28x28\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        #Size 2 = 14x14\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=2)\n",
        "        #Size 3 = 16x16\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        #Size 4 = 8x8\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "        #Size 5 = 8x8\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        #Final Size = 4x4\n",
        "        self.fc1 = nn.Linear(in_features=64 * 4 * 4, out_features=256)\n",
        "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
        "        self.fc3 = nn.Linear(in_features=128, out_features=10)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        scores = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "# IMPORTANT CODE BELOW\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Create an instance of the neural network\n",
        "net = FashionMNISTNet()\n",
        "print(net)\n",
        "# Move the model to the GPU if available\n",
        "net.to(device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RSaAFyUBfbc9",
        "outputId": "17ec6e56-65dc-44d9-dae2-e3f0a5edcaa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "FashionMNISTNet(\n",
              "  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2))\n",
              "  (conv3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=1024, out_features=256, bias=True)\n",
              "  (fc2): Linear(in_features=256, out_features=128, bias=True)\n",
              "  (fc3): Linear(in_features=128, out_features=10, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 2**\n",
        "**Training without Validation Set**\n",
        "* Epochs = 11\n",
        "* Batch Size = 32\n",
        "* Learning Rate in SGD = 0.1\n",
        "* Training Set Size = Full Training Dataset"
      ],
      "metadata": {
        "id": "hKun-sDgVvpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 11\n",
        "# Train on full train dataset\n",
        "train_dataset = full_train_dataset\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train: {len(train_dataset)} | Test: {len(test_dataset)}\")\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(\"Training Epoch: \", epoch + 1)\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / (i + 1)\n",
        "    print(\"Average Loss: \", avg_loss)\n",
        "\n",
        "print(\"Training Finished.\")\n",
        "\n",
        "# Test the neural network\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "net.eval()\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        # Move the inputs and labels to the GPU if available\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # Get the predicted class\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update the total number of samples and correct predictions\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SviF2tNYF70",
        "outputId": "86e39f53-d693-468d-d2a0-aa654ec5ceb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 60000 | Test: 10000\n",
            "Training Epoch:  1\n",
            "Average Loss:  0.6189665213425954\n",
            "Training Epoch:  2\n",
            "Average Loss:  0.3278081944982211\n",
            "Training Epoch:  3\n",
            "Average Loss:  0.26878975222706797\n",
            "Training Epoch:  4\n",
            "Average Loss:  0.23615353941619396\n",
            "Training Epoch:  5\n",
            "Average Loss:  0.21119665132860344\n",
            "Training Epoch:  6\n",
            "Average Loss:  0.19270971008886895\n",
            "Training Epoch:  7\n",
            "Average Loss:  0.1759480779826641\n",
            "Training Epoch:  8\n",
            "Average Loss:  0.15984519085114202\n",
            "Training Epoch:  9\n",
            "Average Loss:  0.1474567214558522\n",
            "Training Epoch:  10\n",
            "Average Loss:  0.13491636175786456\n",
            "Training Epoch:  11\n",
            "Average Loss:  0.12463298247394462\n",
            "Training Finished.\n",
            "Accuracy: 91.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training with Validation Set**"
      ],
      "metadata": {
        "id": "OZT4TzOKZGSL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS\n",
        "#test using TRAIN_SET_SZIE 0.9, 0.8, 0.7, 0.6\n",
        "TRAIN_SET_SZIE = 0.9\n",
        "#TRAIN_SET_SZIE = 0.8\n",
        "#TRAIN_SET_SZIE = 0.7\n",
        "#TRAIN_SET_SZIE = 0.6\n",
        "\n",
        "# Split full_train_dataset into training and validation sets (80%/20%)\n",
        "train_size = int(TRAIN_SET_SZIE * len(full_train_dataset))\n",
        "val_size = len(full_train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")\n",
        "\n",
        "# saving the model with the best validation accuracy\n",
        "best_val_acc = 0.0\n",
        "best_model_path = \"best_fashionmnist_model.pth\"\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(\"Training Epoch: \", epoch + 1)\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / (i + 1)\n",
        "    print(\"Average Loss: \", avg_loss)\n",
        "\n",
        "    # --- Validation ---\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = net(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    val_acc = 100 * correct / total\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(net.state_dict(), best_model_path)\n",
        "        print(f\"✅ New best model saved with val acc: {val_acc:.2f}%\")\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "# Load the saved model and Set the model to evaluation mode\n",
        "net.load_state_dict(torch.load(\"best_fashionmnist_model.pth\"))\n",
        "net.eval()\n",
        "\n",
        "# Test the neural network\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "net.eval()\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        # Move the inputs and labels to the GPU if available\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # Get the predicted class\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update the total number of samples and correct predictions\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "bzDJl2CnYx98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results from training the model without a validation set:<br>\n",
        "Train: 60000 | Test: 10000<br>\n",
        "\n",
        "Training Epoch:  1<br>\n",
        "Average Loss:  0.654186999575297<br>\n",
        "Training Epoch:  2<br>\n",
        "Average Loss:  0.31929630262851716<br>\n",
        "Training Epoch:  3<br>\n",
        "Average Loss:  0.2617528966650367<br>\n",
        "Training Epoch:  4<br>\n",
        "Average Loss:  0.2281300640384356<br>\n",
        "Training Epoch:  5<br>\n",
        "Average Loss:  0.2042231744448344<br>\n",
        "Training Epoch:  6<br>\n",
        "Average Loss:  0.18343615941877167<br>\n",
        "Training Epoch:  7<br>\n",
        "Average Loss:  0.16709979757765928<br>\n",
        "Training Epoch:  8<br>\n",
        "Average Loss:  0.15363689922106763<br>\n",
        "Training Epoch:  9<br>\n",
        "Average Loss:  0.14050291257823508<br>\n",
        "Training Epoch:  10<br>\n",
        "Average Loss:  0.12784666278598208<br>\n",
        "Training Epoch:  11<br>\n",
        "Average Loss:  0.11665267256665975<br>\n",
        "\n",
        "Test Accuracy Result:<br>\n",
        "Accuracy: 91.75%<br>"
      ],
      "metadata": {
        "id": "ELe_jXUZU8MT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Validation of Model using 90%, 10% split between training and validation data:<br>\n",
        "\n",
        "Train: 54000 | Val: 6000 | Test: 10000 <br>\n",
        "\n",
        "✅ New best model saved with val acc: 87.17%<br>\n",
        "Epoch [1/11], Loss: 0.6349, Val Acc: 87.17%<br>\n",
        "✅ New best model saved with val acc: 87.58%<br>\n",
        "Epoch [2/11], Loss: 0.3328, Val Acc: 87.58%<br>\n",
        "✅ New best model saved with val acc: 89.93%<br>\n",
        "Epoch [3/11], Loss: 0.2759, Val Acc: 89.93%<br>\n",
        "✅ New best model saved with val acc: 90.45%<br>\n",
        "Epoch [4/11], Loss: 0.2425, Val Acc: 90.45%<br>\n",
        "Epoch [5/11], Loss: 0.2170, Val Acc: 90.10%<br>\n",
        "✅ New best model saved with val acc: 91.83%<br>\n",
        "Epoch [6/11], Loss: 0.1965, Val Acc: 91.83%<br>\n",
        "Epoch [7/11], Loss: 0.1766, Val Acc: 91.00%<br>\n",
        "Epoch [8/11], Loss: 0.1618, Val Acc: 91.10%<br>\n",
        "Epoch [9/11], Loss: 0.1483, Val Acc: 91.50%<br>\n",
        "Epoch [10/11], Loss: 0.1365, Val Acc: 91.42%<br>\n",
        "✅ New best model saved with val acc: 92.08%<br>\n",
        "Epoch [11/11], Loss: 0.1228, Val Acc: 92.08%<br>\n",
        "Training finished.<br>\n",
        "Best Validation Accuracy: 92.08%<br>\n",
        "Accuracy: 91.83%"
      ],
      "metadata": {
        "id": "_SrIzGJgkvpI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Validation of Model using 80%, 20% split between training and validation data:<br>\n",
        "\n",
        "Train: 48000 | Val: 12000 | Test: 10000<br>\n",
        "\n",
        "✅ New best model saved with val acc: 86.15%<br>\n",
        "Epoch [1/11], Loss: 0.6720, Val Acc: 86.15%<br>\n",
        "✅ New best model saved with val acc: 88.38%<br>\n",
        "Epoch [2/11], Loss: 0.3396, Val Acc: 88.38%<br>\n",
        "Epoch [3/11], Loss: 0.2813, Val Acc: 87.63%<br>\n",
        "✅ New best model saved with val acc: 89.60%<br>\n",
        "Epoch [4/11], Loss: 0.2476, Val Acc: 89.60%<br>\n",
        "✅ New best model saved with val acc: 90.53%<br>\n",
        "Epoch [5/11], Loss: 0.2202, Val Acc: 90.53%<br>\n",
        "✅ New best model saved with val acc: 91.31%<br>\n",
        "Epoch [6/11], Loss: 0.2002, Val Acc: 91.31%<br>\n",
        "Epoch [7/11], Loss: 0.1795, Val Acc: 91.14%<br>\n",
        "Epoch [8/11], Loss: 0.1652, Val Acc: 91.13%<br>\n",
        "✅ New best model saved with val acc: 91.47%<br>\n",
        "Epoch [9/11], Loss: 0.1499, Val Acc: 91.47%<br>\n",
        "✅ New best model saved with val acc: 91.52%<br>\n",
        "Epoch [10/11], Loss: 0.1373, Val Acc: 91.52%<br>\n",
        "Epoch [11/11], Loss: 0.1260, Val Acc: 91.15%<br>\n",
        "Training finished.<br>\n",
        "Best Validation Accuracy: 91.52%<br>\n",
        "Accuracy: 91.24%\n"
      ],
      "metadata": {
        "id": "EzU2aX87nhC-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Validation of Model using 70%, 30% split between training and validation data:<br>\n",
        "Train: 42000 | Val: 18000 | Test: 10000<br>\n",
        "\n",
        "✅ New best model saved with val acc: 83.73%<br>\n",
        "Epoch [1/11], Loss: 0.6996, Val Acc: 83.73%<br>\n",
        "Epoch [2/11], Loss: 0.3495, Val Acc: 83.53%<br>\n",
        "✅ New best model saved with val acc: 88.91%<br>\n",
        "Epoch [3/11], Loss: 0.2893, Val Acc: 88.91%<br>\n",
        "✅ New best model saved with val acc: 90.19%<br>\n",
        "Epoch [4/11], Loss: 0.2530, Val Acc: 90.19%<br>\n",
        "Epoch [5/11], Loss: 0.2265, Val Acc: 88.84%<br>\n",
        "Epoch [6/11], Loss: 0.2081, Val Acc: 90.19%<br>\n",
        "Epoch [7/11], Loss: 0.1893, Val Acc: 90.09%<br>\n",
        "✅ New best model saved with val acc: 91.13%<br>\n",
        "Epoch [8/11], Loss: 0.1743, Val Acc: 91.13%<br>\n",
        "Epoch [9/11], Loss: 0.1583, Val Acc: 90.46%<br>\n",
        "✅ New best model saved with val acc: 91.18%<br>\n",
        "Epoch [10/11], Loss: 0.1482, Val Acc: 91.18%<br>\n",
        "Epoch [11/11], Loss: 0.1325, Val Acc: 90.88%<br>\n",
        "Training finished.<br>\n",
        "Best Validation Accuracy: 91.18%<br>\n",
        "Accuracy: 91.08%"
      ],
      "metadata": {
        "id": "aw00VO8rvIZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and Validation of Model using 60%, 40% split between training and validation data:<br>\n",
        "\n",
        "Train: 36000 | Val: 24000 | Test: 10000<br>\n",
        "\n",
        "✅ New best model saved with val acc: 84.08%<br>\n",
        "Epoch [1/11], Loss: 0.8012, Val Acc: 84.08%<br>\n",
        "✅ New best model saved with val acc: 86.35%<br>\n",
        "Epoch [2/11], Loss: 0.3829, Val Acc: 86.35%<br>\n",
        "✅ New best model saved with val acc: 89.65%<br>\n",
        "Epoch [3/11], Loss: 0.3174, Val Acc: 89.65%<br>\n",
        "Epoch [4/11], Loss: 0.2754, Val Acc: 89.34%<br>\n",
        "✅ New best model saved with val acc: 89.89%<br>\n",
        "Epoch [5/11], Loss: 0.2441, Val Acc: 89.89%<br>\n",
        "✅ New best model saved with val acc: 90.22%<br>\n",
        "Epoch [6/11], Loss: 0.2236, Val Acc: 90.22%<br>\n",
        "✅ New best model saved with val acc: 91.36%<br>\n",
        "Epoch [7/11], Loss: 0.2039, Val Acc: 91.36%<br>\n",
        "Epoch [8/11], Loss: 0.1863, Val Acc: 91.34%<br>\n",
        "Epoch [9/11], Loss: 0.1686, Val Acc: 91.28%<br>\n",
        "✅ New best model saved with val acc: 91.93%<br>\n",
        "Epoch [10/11], Loss: 0.1550, Val Acc: 91.93%<br>\n",
        "Epoch [11/11], Loss: 0.1396, Val Acc: 91.13%<br>\n",
        "Training finished.<br>\n",
        "Best Validation Accuracy: 91.93%<br>\n",
        "Accuracy: 91.14%<br>"
      ],
      "metadata": {
        "id": "22l0xE--PyZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Findings and analysis:<br>**\n",
        "Based on the results of running the training at different percentages, it seemed the best accuracy came from using a 90% training data - 10% validation data split. It is worth noting that the variations had relatively close validation accuracy, and it's possible that with repeated trials or with different values for batch sizing and number of epochs to have different results. It'd be worth trying with different values and multiple runs to get a clearer result and a statistically proveable result.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "Cwebx-yKTLD8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 3<br>**\n",
        "Batch Size                  = 32<br>\n",
        "Number of Epochs            = 11<br>\n",
        "Training - Validation Split = 90% - 10%<br>\n",
        "Train: 54000 | Val: 6000 | Test: 10000<br>\n",
        "Learning Rate stepped between 0.001, 0.01, 0.1, 1, 10\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "aCq2-woIVcEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_SET_SZIE = 0.9\n",
        "NUM_EPOCHS = 11\n",
        "# Split full_train_dataset into training and validation sets (80%/20%)\n",
        "train_size = int(TRAIN_SET_SZIE * len(full_train_dataset))\n",
        "val_size = len(full_train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")\n",
        "\n",
        "# saving the model with the best validation accuracy\n",
        "best_val_acc = 0.0\n",
        "best_model_path = \"best_fashionmnist_model.pth\"\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "#test with LEARNING_RATE = 0.001, 0.01, 0.1, 1, 10\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(\"Training Epoch: \", epoch + 1)\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / (i + 1)\n",
        "    print(\"Average Loss: \", avg_loss)\n",
        "\n",
        "    # --- Validation ---\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = net(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    val_acc = 100 * correct / total\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(net.state_dict(), best_model_path)\n",
        "        print(f\"✅ New best model saved with val acc: {val_acc:.2f}%\")\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "# Load the saved model and Set the model to evaluation mode\n",
        "net.load_state_dict(torch.load(\"best_fashionmnist_model.pth\"))\n",
        "net.eval()\n",
        "\n",
        "# Test the neural network\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "net.eval()\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        # Move the inputs and labels to the GPU if available\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # Get the predicted class\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update the total number of samples and correct predictions\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "id": "-SA6Dkecb-Zd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model using a Learning rate of 0.001:<br>\n",
        "\n",
        "✅ New best model saved with val acc: 19.65%<br>\n",
        "Epoch [1/11], Loss: 2.3002, Val Acc: 19.65%<br>\n",
        "✅ New best model saved with val acc: 21.00%<br>\n",
        "Epoch [2/11], Loss: 2.2907, Val Acc: 21.00%<br>\n",
        "✅ New best model saved with val acc: 42.87%<br>\n",
        "Epoch [3/11], Loss: 2.2651, Val Acc: 42.87%<br>\n",
        "✅ New best model saved with val acc: 49.98%<br>\n",
        "Epoch [4/11], Loss: 2.0634, Val Acc: 49.98%<br>\n",
        "✅ New best model saved with val acc: 68.20%<br>\n",
        "Epoch [5/11], Loss: 1.1677, Val Acc: 68.20%<br>\n",
        "✅ New best model saved with val acc: 72.37%<br>\n",
        "Epoch [6/11], Loss: 0.7959, Val Acc: 72.37%<br>\n",
        "✅ New best model saved with val acc: 72.60%<br>\n",
        "Epoch [7/11], Loss: 0.7046, Val Acc: 72.60%<br>\n",
        "✅ New best model saved with val acc: 76.15%<br>\n",
        "Epoch [8/11], Loss: 0.6602, Val Acc: 76.15%<br>\n",
        "✅ New best model saved with val acc: 76.98%<br>\n",
        "Epoch [9/11], Loss: 0.6304, Val Acc: 76.98%<br>\n",
        "✅ New best model saved with val acc: 77.68%<br>\n",
        "Epoch [10/11], Loss: 0.6078, Val Acc: 77.68%<br>\n",
        "Epoch [11/11], Loss: 0.5908, Val Acc: 77.33%<br>\n",
        "Training finished.<br>\n",
        "Best Validation Accuracy: 77.68%<br>\n",
        "Accuracy: 77.28%<br>"
      ],
      "metadata": {
        "id": "7akPQJkOW4P6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model using a Learning rate of 0.01:<br>\n",
        "\n",
        "✅ New best model saved with val acc: 73.82%<br>\n",
        "Epoch [1/11], Loss: 1.5512, Val Acc: 73.82%<br>\n",
        "✅ New best model saved with val acc: 81.67%<br>\n",
        "Epoch [2/11], Loss: 0.5830, Val Acc: 81.67%<br>\n",
        "Epoch [3/11], Loss: 0.4789, Val Acc: 80.05%<br>\n",
        "✅ New best model saved with val acc: 84.32%<br>\n",
        "Epoch [4/11], Loss: 0.4188, Val Acc: 84.32%<br>\n",
        "✅ New best model saved with val acc: 84.70%<br>\n",
        "Epoch [5/11], Loss: 0.3814, Val Acc: 84.70%<br>\n",
        "✅ New best model saved with val acc: 87.35%<br>\n",
        "Epoch [6/11], Loss: 0.3529, Val Acc: 87.35%<br>\n",
        "Epoch [7/11], Loss: 0.3318, Val Acc: 86.98%<br>\n",
        "✅ New best model saved with val acc: 88.92%<br>\n",
        "Epoch [8/11], Loss: 0.3132, Val Acc: 88.92%<br>\n",
        "Epoch [9/11], Loss: 0.2999, Val Acc: 88.87%<br>\n",
        "Epoch [10/11], Loss: 0.2867, Val Acc: 88.40%<br>\n",
        "✅ New best model saved with val acc: 89.05%<br>\n",
        "Epoch [11/11], Loss: 0.2750, Val Acc: 89.05%<br>\n",
        "Training finished.<br>\n",
        "Best Validation Accuracy: 89.05%<br>\n",
        "Accuracy: 88.41%<br>"
      ],
      "metadata": {
        "id": "iQB3cjY9X8aY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model using a Learning rate of 0.1:<br>\n",
        "\n",
        "✅ New best model saved with val acc: 84.95%<br>\n",
        "Epoch [1/11], Loss: 0.6647, Val Acc: 84.95%<br>\n",
        "✅ New best model saved with val acc: 89.23%<br>\n",
        "Epoch [2/11], Loss: 0.3356, Val Acc: 89.23%<br>\n",
        "✅ New best model saved with val acc: 89.65%<br>\n",
        "Epoch [3/11], Loss: 0.2753, Val Acc: 89.65%<br>\n",
        "✅ New best model saved with val acc: 90.50%<br>\n",
        "Epoch [4/11], Loss: 0.2407, Val Acc: 90.50%<br>\n",
        "✅ New best model saved with val acc: 91.17%<br>\n",
        "Epoch [5/11], Loss: 0.2161, Val Acc: 91.17%<br>\n",
        "✅ New best model saved with val acc: 91.40%<br>\n",
        "Epoch [6/11], Loss: 0.1938, Val Acc: 91.40%<br>\n",
        "Epoch [7/11], Loss: 0.1775, Val Acc: 88.27%<br>\n",
        "✅ New best model saved with val acc: 92.28%<br>\n",
        "Epoch [8/11], Loss: 0.1624, Val Acc: 92.28%<br>\n",
        "Epoch [9/11], Loss: 0.1458, Val Acc: 91.32%<br>\n",
        "Epoch [10/11], Loss: 0.1362, Val Acc: 91.77%<br>\n",
        "✅ New best model saved with val acc: 92.38%<br>\n",
        "Epoch [11/11], Loss: 0.1227, Val Acc: 92.38%<br>\n",
        "Training finished.<br>\n",
        "Best Validation Accuracy: 92.38%<br>\n",
        "Accuracy: 91.39%<br>"
      ],
      "metadata": {
        "id": "Izc5BvbVZpyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model using a Learning rate of 1:<br>\n",
        "\n",
        "✅ New best model saved with val acc: 9.57%<br>\n",
        "Epoch [1/11], Loss: 2.3073, Val Acc: 9.57%<br>\n",
        "✅ New best model saved with val acc: 10.53%<br>\n",
        "Epoch [2/11], Loss: 2.3105, Val Acc: 10.53%<br>\n",
        "Epoch [3/11], Loss: 2.3101, Val Acc: 9.60%<br>\n",
        "Epoch [4/11], Loss: 2.3102, Val Acc: 9.98%<br>\n",
        "Epoch [5/11], Loss: 2.3100, Val Acc: 9.95%<br>\n",
        "Epoch [6/11], Loss: 2.3107, Val Acc: 10.00%<br>\n",
        "Epoch [7/11], Loss: 2.3101, Val Acc: 10.10%<br>\n",
        "Epoch [8/11], Loss: 2.3100, Val Acc: 9.95%<br>\n",
        "Epoch [9/11], Loss: 2.3104, Val Acc: 10.13%<br>\n",
        "Epoch [10/11], Loss: 2.3094, Val Acc: 10.00%<br>\n",
        "Epoch [11/11], Loss: 2.3096, Val Acc: 9.57%<br>\n",
        "Training finished.<br>\n",
        "Best Validation Accuracy: 10.53%<br>\n",
        "Accuracy: 10.00%"
      ],
      "metadata": {
        "id": "K2Y370ZbZssV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training the model using a Learning rate of 10:<br>\n",
        "\n",
        "✅ New best model saved with val acc: 10.25%<br>\n",
        "Epoch [1/11], Loss: 2.4786, Val Acc: 10.25%<br>\n",
        "Epoch [2/11], Loss: 2.4661, Val Acc: 9.80%<br>\n",
        "Epoch [3/11], Loss: 2.4739, Val Acc: 9.43%<br>\n",
        "Epoch [4/11], Loss: 2.4743, Val Acc: 9.97%<br>\n",
        "Epoch [5/11], Loss: 2.4753, Val Acc: 9.80%<br>\n",
        "Epoch [6/11], Loss: 2.4688, Val Acc: 10.03%<br>\n",
        "Epoch [7/11], Loss: 2.4680, Val Acc: 9.80%<br>\n",
        "Epoch [8/11], Loss: 2.4697, Val Acc: 9.97%<br>\n",
        "Epoch [9/11], Loss: 2.4786, Val Acc: 10.22%<br>\n",
        "Epoch [10/11], Loss: 2.4742, Val Acc: 10.25%<br>\n",
        "Epoch [11/11], Loss: 2.4696, Val Acc: 9.90%<br>\n",
        "Training finished.<br>\n",
        "Best Validation Accuracy: 10.25%<br>\n",
        "Accuracy: 10.00%"
      ],
      "metadata": {
        "id": "ZiYC4_RXZvxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Findings and Analysis:<br>**\n",
        "While it started off quite weak, the results with LR = 0.001 improved surprisingly quickly. Although they began to taper off and finish in the high 70% range, it did continue to increase through the Epochs. If the improvements towards the optimum were faster, it would be a useful rate. Unfortunately, the speed at which LR = 0.01 and 0.1 approached an optimum value far exceed the time required for LR = 0.001 to be feasible.<br>\n",
        "The results from LR = 0.01 were impressive, and seem like it would benefit from a having a large Epoch number, as it was still making fairly consititent improvements towards the 90% range. I wonder if there is a point where this rate competes with LR = 0.1 in terms of accuracy vs time.<br>\n",
        "LR = 0.1 performed the strongest out of the 5 tests. It started at quite a high percentage rate, and showed consistent improvements through the 11 Epochs. I'm curious to see how the value would perform at different sizes of Batches and/or Epochs.<br>\n",
        "LR = 1 and LR = 10 both had similar , and poor, results. They repeatedly bounced around 9-10% accuracy, almost alternating in a pendulum fashion. This clearly demonstrated the information discussed in Lecture 23 slide 31, where having too high of a step size will jump over the optimum value.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "50solZqLb7EQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 4**<br>\n",
        "\n",
        "Changing to the ADAM training algorithm.<br>\n",
        "Batch Size                  = 32<br>\n",
        "Number of Epochs            = 11<br>\n",
        "Training - Validation Split = 90% - 10%<br>\n",
        "Train: 54000 | Val: 6000 | Test: 10000<br>\n",
        "Learning Rate               = 0.1<br>\n",
        "\n",
        "Prediction: I expect using the Adam training algorithm to return improved results. The adaptive nature should lead to improvements in comparrison to SGD.\n"
      ],
      "metadata": {
        "id": "MKHnqJBeg2MW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "outputId": "d4faebaf-feb9-473b-df72-e54f83f1aec6",
        "id": "OoA66fgd-BqX"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ New best model saved with val acc: 9.88%\n",
            "Epoch [1/11], Loss: 16.5248, Val Acc: 9.88%\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-adbcc556f54a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "NUM_EPOCHS = 11\n",
        "# Define the loss function and the optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.1)\n",
        "\n",
        "# saving the model with the best validation accuracy\n",
        "best_val_acc = 0.0\n",
        "best_model_path = \"best_fashionmnist_model.pth\"\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / (i + 1)\n",
        "\n",
        "    # --- Validation ---\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = net(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    val_acc = 100 * correct / total\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(net.state_dict(), best_model_path)\n",
        "        print(f\"✅ New best model saved with val acc: {val_acc:.2f}%\")\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "# Test the neural network\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "# Load the saved model and Set the model to evaluation mode\n",
        "#net.load_state_dict(torch.load(\"best_fashionmnist_model.pth\"))\n",
        "net.eval()\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        # Move the inputs and labels to the GPU if available\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # Get the predicted class\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update the total number of samples and correct predictions\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Results of using Adam training algorithm with the same parameters as SGD:<br>\n",
        "\n",
        "Train: 54000 | Val: 6000 | Test: 10000<br>\n",
        "\n",
        "✅ New best model saved with val acc: 9.57%<br>\n",
        "Epoch [1/11], Loss: 73.4058, Val Acc: 9.57%<br>\n",
        "✅ New best model saved with val acc: 10.47%<br>\n",
        "Epoch [2/11], Loss: 2.3154, Val Acc: 10.47%<br>\n",
        "Epoch [3/11], Loss: 2.3150, Val Acc: 9.57%<br>\n",
        "Epoch [4/11], Loss: 2.3150, Val Acc: 9.65%<br>\n",
        "Epoch [5/11], Loss: 2.3158, Val Acc: 9.57%<br>\n",
        "✅ New best model saved with val acc: 10.58%<br>\n",
        "Epoch [6/11], Loss: 2.3153, Val Acc: 10.58%<br>\n",
        "Epoch [7/11], Loss: 2.3155, Val Acc: 9.57%<br>\n",
        "✅ New best model saved with val acc: 11.00%<br>\n",
        "Epoch [8/11], Loss: 2.3149, Val Acc: 11.00%<br>\n",
        "Epoch [9/11], Loss: 2.3148, Val Acc: 9.57%<br>\n",
        "Epoch [10/11], Loss: 2.3158, Val Acc: 10.47%<br>\n",
        "Epoch [11/11], Loss: 2.3154, Val Acc: 9.35%<br>\n",
        "Training finished.<br>\n",
        "Best Validation Accuracy: 11.00%<br>\n",
        "Accuracy: 10.00% <br>\n",
        "\n",
        "I am surprised by these results! I have even run this a few times to verify I had it set up correct. These results are very similar to the results I found when running SGD with a higher learning rate. <br>\n",
        "\n",
        "**I decided to run the model with ADAM at LR = 0.001:<br>**\n",
        "\n",
        "Train: 54000 | Val: 6000 | Test: 10000<br>\n",
        "\n",
        "✅ New best model saved with val acc: 87.97%<br>\n",
        "Epoch [1/11], Loss: 0.4663, Val Acc: 87.97%<br>\n",
        "✅ New best model saved with val acc: 88.85%<br>\n",
        "Epoch [2/11], Loss: 0.2876, Val Acc: 88.85%<br>\n",
        "✅ New best model saved with val acc: 91.57%<br>\n",
        "Epoch [3/11], Loss: 0.2425, Val Acc: 91.57%<br>\n",
        "Epoch [4/11], Loss: 0.2119, Val Acc: 91.50%<br>\n",
        "✅ New best model saved with val acc: 92.00%<br>\n",
        "Epoch [5/11], Loss: 0.1879, Val Acc: 92.00%<br>\n",
        "Epoch [6/11], Loss: 0.1670, Val Acc: 91.70%<br>\n",
        "Epoch [7/11], Loss: 0.1503, Val Acc: 91.67%<br>\n",
        "✅ New best model saved with val acc: 92.25%<br>\n",
        "Epoch [8/11], Loss: 0.1307, Val Acc: 92.25%<br>\n",
        "Epoch [9/11], Loss: 0.1182, Val Acc: 92.18%<br>\n",
        "Epoch [10/11], Loss: 0.1046, Val Acc: 91.98%<br>\n",
        "Epoch [11/11], Loss: 0.0951, Val Acc: 91.65%<br>\n",
        "Training finished.<br>\n",
        "Best Validation Accuracy: 92.25%<br>\n",
        "Accuracy: 91.66%<br>\n",
        "\n",
        "These results are closer to what I expected when I began. It seems that ADAM begins running into the \"looping\" behavior at a much lower rate than SGD does. I suppose that is why it is preferred. It would allow for the ability to make smaller adjustments while maintaining the same speed, so it should generally have a higher overall accuracy than SGD can reliably attain.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gWeZSco1F4GN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Task 4**\n",
        "\n",
        "Using AUROC to evaluate model performance of CNN trained with ADAM<br>\n",
        "\n",
        "Batch Size                  = 32<br>\n",
        "Number of Epochs            = 11<br>\n",
        "Training - Validation Split = 90% - 10%<br>\n",
        "Train: 54000 | Val: 6000 | Test: 10000<br>\n",
        "Learning Rate               = 0.001<br>\n",
        "I decided to change the learning rate to 0.001 and to use the ADAM training algorithm, as it seemed to consistently get an equal or higher accuracy compared to SGD.<br>\n",
        "I think the AUROC results will be decently high, maybe high 80%-mid 90%'s, because the model has a fairly high accuracy rate overall.\n"
      ],
      "metadata": {
        "id": "8Q1JX5Md4KEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the positive class\n",
        "#The other classes will be negative\n",
        "TRUTH_CLASS = 2\n",
        "\n",
        "# Test the neural network\n",
        "correct = 0\n",
        "total = 0\n",
        "auc_metric = BinaryAUROC()\n",
        "\n",
        "# Load the saved model and Set the model to evaluation mode\n",
        "net.load_state_dict(torch.load(\"best_fashionmnist_model.pth\"))\n",
        "net.eval()\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        # Move the inputs and labels to the GPU if available\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # Get the predicted class\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update the total number of samples and correct predictions\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # --- AUROC ---\n",
        "        binary_ground_truth = (labels == TRUTH_CLASS).float()\n",
        "        binary_predictions = outputs[:, TRUTH_CLASS]\n",
        "\n",
        "        auc_metric.update(binary_predictions, binary_ground_truth)\n",
        "\n",
        "#compute Area Under Reciever Operating Characteristic Curve\n",
        "auc_final = auc_metric.compute()\n",
        "print(f\"Area Under Curve Results for: {auc_final:.2f}\")\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2kSagdLi4HtM",
        "outputId": "6f1ce272-86c8-4c99-8915-b43a882ecb09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torcheval in /usr/local/lib/python3.11/dist-packages (0.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torcheval) (4.13.2)\n",
            "Area Under Curve Results for: 0.96\n",
            "Accuracy: 91.29%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Results and Analysis**<br>\n",
        "\n",
        "Area Under Curve Results: 0.99<br>\n",
        "Accuracy: 91.29%<br>\n",
        "\n",
        "I am not sure that I have implemented the AUROC correctly. A value of 0.99 with an accuracy of 91.29% just seems too high. Since the AUROC is testing for how many true positives are flagged in comparison to false positives, I suppose it makes sense that it would be able to fairly reliable not mislabel a particular class. <br>\n",
        "I'd be interested in seeing the results for all classes to see if this holds across them.<br>\n",
        "I tried a few other classes, and most were similarly high. Here is a result from setting 6 as the positive class:<br>\n",
        "Area Under Curve Results for: 0.96<br>\n",
        "Accuracy: 91.29%<br>"
      ],
      "metadata": {
        "id": "I3DeGYwaClsh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Final Build of CNN using ADAM and AUC**\n",
        "*   Batch Size = 32\n",
        "*   Epochs = 11\n",
        "*   Learning Rate in SGD = 0.001\n",
        "*   Positive/Truth Class = 2\n",
        "*   Training Data - Validation Data split: 90% - 10% (based on trials from 90% - 10% to 60% - 40%)<br>\n",
        "Split between Training and Validation data can be altered by setting TRAIN_SET_SIZE value.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "FCWFNTVPQDyf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3HKY8l_iPWx",
        "outputId": "b163e28d-7813-474f-fc18-e2eb1fe0a758"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torcheval in /usr/local/lib/python3.11/dist-packages (0.0.7)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from torcheval) (4.13.2)\n",
            "Train: 54000 | Val: 6000 | Test: 10000\n",
            "Using device: cuda\n",
            "✅ New best model saved with val acc: 88.53%\n",
            "Epoch [1/11], Loss: 0.4795, Val Acc: 88.53%\n",
            "✅ New best model saved with val acc: 90.10%\n",
            "Epoch [2/11], Loss: 0.2892, Val Acc: 90.10%\n",
            "✅ New best model saved with val acc: 91.43%\n",
            "Epoch [3/11], Loss: 0.2397, Val Acc: 91.43%\n",
            "Epoch [4/11], Loss: 0.2078, Val Acc: 91.32%\n",
            "Epoch [5/11], Loss: 0.1849, Val Acc: 91.00%\n",
            "✅ New best model saved with val acc: 91.62%\n",
            "Epoch [6/11], Loss: 0.1619, Val Acc: 91.62%\n",
            "✅ New best model saved with val acc: 92.03%\n",
            "Epoch [7/11], Loss: 0.1465, Val Acc: 92.03%\n",
            "✅ New best model saved with val acc: 92.27%\n",
            "Epoch [8/11], Loss: 0.1314, Val Acc: 92.27%\n",
            "✅ New best model saved with val acc: 92.57%\n",
            "Epoch [9/11], Loss: 0.1139, Val Acc: 92.57%\n",
            "Epoch [10/11], Loss: 0.1029, Val Acc: 92.50%\n",
            "Epoch [11/11], Loss: 0.0929, Val Acc: 92.53%\n",
            "Training finished.\n",
            "Best Validation Accuracy: 92.57%\n",
            "Area Under Curve Results: 0.99\n",
            "Accuracy: 91.29%\n"
          ]
        }
      ],
      "source": [
        "!pip install torcheval\n",
        "from torcheval.metrics import BinaryAUROC\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "\n",
        "#Precentage of the training data used for Training\n",
        "#Precentage of the validation set = 1 - TRAIN_SET_SZIE\n",
        "TRAIN_SET_SZIE = 0.9\n",
        "#Define minibatch size\n",
        "BATCH_VAL = 128\n",
        "#Define the positive class\n",
        "#The other classes will be negative\n",
        "TRUTH_CLASS = 2\n",
        "NUM_EPOCHS = 11\n",
        "\n",
        "# Define data transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(28),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Load the FashionMNIST dataset\n",
        "full_train_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data', train=True, download=True, transform=transform\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.FashionMNIST(\n",
        "    root='./data', train=False, download=True, transform=transform\n",
        ")\n",
        "\n",
        "# Split full_train_dataset into training and validation sets (80%/20%)\n",
        "train_size = int(TRAIN_SET_SZIE * len(full_train_dataset))\n",
        "val_size = len(full_train_dataset) - train_size\n",
        "train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_VAL, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_VAL, shuffle=False, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_VAL, shuffle=False, num_workers=2)\n",
        "\n",
        "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")\n",
        "\n",
        "# Define the neural network for FashionMNIST\n",
        "class FashionMNISTNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FashionMNISTNet, self).__init__()\n",
        "        #Initial size 28x28\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1)\n",
        "        #Size 1 = 28x28\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        #Size 2 = 14x14\n",
        "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=2)\n",
        "        #Size 3 = 16x16\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        #Size 4 = 8x8\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "        #Size 5 = 8x8\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        #Final Size = 4x4\n",
        "        self.fc1 = nn.Linear(in_features=64 * 4 * 4, out_features=256)\n",
        "        self.fc2 = nn.Linear(in_features=256, out_features=128)\n",
        "        self.fc3 = nn.Linear(in_features=128, out_features=10)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.conv1(x)))\n",
        "        x = self.pool(F.relu(self.conv2(x)))\n",
        "        x = self.pool(F.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 4 * 4)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        scores = self.softmax(x)\n",
        "        return x\n",
        "\n",
        "# IMPORTANT CODE BELOW\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# Create an instance of the neural network\n",
        "net = FashionMNISTNet()\n",
        "\n",
        "# Move the model to the GPU if available\n",
        "net.to(device)\n",
        "\n",
        "# Define the loss function and the optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
        "\n",
        "# saving the model with the best validation accuracy\n",
        "best_val_acc = 0.0\n",
        "best_model_path = \"best_fashionmnist_model.pth\"\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    net.train()\n",
        "    running_loss = 0.0\n",
        "    for i, (inputs, labels) in enumerate(train_loader, 0):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    avg_loss = running_loss / (i + 1)\n",
        "\n",
        "    # --- Validation ---\n",
        "    net.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in val_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = net(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    val_acc = 100 * correct / total\n",
        "\n",
        "    # Save best model\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        torch.save(net.state_dict(), best_model_path)\n",
        "        print(f\"✅ New best model saved with val acc: {val_acc:.2f}%\")\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "print(\"Training finished.\")\n",
        "print(f\"Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
        "\n",
        "# Test the neural network\n",
        "correct = 0\n",
        "total = 0\n",
        "auc_metric = BinaryAUROC()\n",
        "\n",
        "# Load the saved model and Set the model to evaluation mode\n",
        "net.load_state_dict(torch.load(\"best_fashionmnist_model.pth\"))\n",
        "net.eval()\n",
        "\n",
        "# Disable gradient calculation\n",
        "with torch.no_grad():\n",
        "    for inputs, labels in test_loader:\n",
        "\n",
        "        # Move the inputs and labels to the GPU if available\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        # Get the predicted class\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        # Update the total number of samples and correct predictions\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # --- AUROC ---\n",
        "        binary_ground_truth = (labels == TRUTH_CLASS).float()\n",
        "        binary_predictions = outputs[:, TRUTH_CLASS]\n",
        "        auc_metric.update(binary_predictions, binary_ground_truth)\n",
        "\n",
        "#Calculate AUROC results\n",
        "auc_results = auc_metric.compute()\n",
        "print(f\"Area Under Curve Results: {auc_results:.2f}\")\n",
        "# Calculate the accuracy\n",
        "accuracy = 100 * correct / total\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")"
      ]
    }
  ]
}
